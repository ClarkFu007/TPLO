# Commands for Truth is Universal
conda activate truth_is_universal

## Generate associated activation vectors from given datasets.
This repository provides all datasets used in the paper, but not 
the associated activation vectors due to their large size. You'll 
need to generate these activations before running any other code. 
This requires model weights from the Llama3, Llama2, Gemma 
or Gemma2 ((torch.bfloat16 precision is needed) ) model family. 
We suggest obtaining these weights from https://huggingface.co/ 
(e.g. Llama3-8B-Instruct https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct). 
Insert  the paths to the weight folders into `config.ini`.  Then, run 
`generate_acts.py` to generate the activations. 

For example, to generate activations for the cities and neg_cities datasets 
for Llama3-8B-Instruct in layers 11 and 12:
```
python3 generate_acts.py \
--model_family Llama2 \
--model_size 7B \
--model_type base \
--layers -1 \
--datasets all_topic_specific \
--device cuda:3
python3 generate_acts.py \
--model_family Llama2 \
--model_size 7B \
--model_type base \
--layers -1 \
--datasets all \
--device cuda:3
python3 generate_acts.py \
--model_family Llama3 \
--model_size 8B \
--model_type chat \
--layers 11 12 \
--datasets cities neg_cities \
--device cuda:0
```
For Llama3-8b-chat experiments: 
```
python3 generate_acts.py \
--pruning True \
--pruning_ratio 0.7 \
--model_family Llama3 \
--model_size 8B \
--model_type chat \
--layers -1 \
--datasets all_topic_specific \
--device cuda:3
```
The activations will be stored in the `acts` folder. You can generate the 
activations for all layers by setting `--layers -1`. You can generate the 
activations for all topics-specific datasets (defined in the paper) by 
setting `--datasets all_topic_specific` and for all datasets 
by setting `--datasets all`.

## Repository Structure
Jupyter Notebooks:

* `truth_directions.ipynb`: Code for generating figures from the first 
four paper sections; from learning truth directions to exploring the 
dimensionality of the truth subspace. You need to generate the following 
activations to run this notebook (e.g. for Llama3-8B-Instruct):

* Generate the activations for all topics-specific datasets (defined in 
the paper).
```
python3 generate_acts.py \
--model_family Llama3 \
--model_size 8B \
--model_type chat \
--layers 12 \
--datasets all_topic_specific \
--device cuda:0
```
* and generate the activations for all layers by setting `--layers -1`
for given datasets.
```
python3 generate_acts.py \
--model_family Llama3 \
--model_size 8B \
--model_type chat \
--layers -1 \
--datasets cities neg_cities sp_en_trans neg_sp_en_trans \
--device cuda:0
```

* `generate_lies.ipynb`: For generating the LLM responses (lies) to the real 
world scenarios. Responses generated by Llama3-8B-Instruct are already in the 
datasets folder and have been manually categorized by the first author as 
either honest reply or lie.

* `lie_detection.ipynb`: Three classifiers (TTPD, LR and CCS) are used to 
classify statements as true or false based on the internal LLM activations. 
We examine their ability to generalize to unseen topics, unseen types of statements, 
and real-world lies. This code reproduces the results of Section 5 of the paper. 
The activations of all datasets in the datasets folder (in one layer) are needed 
to run this notebook. You can generate these activations, e.g. for Llama3-8B-Instruct, 
via the following command: 
```
python3 generate_acts.py \
--model_family Llama3 \
--model_size 8B \
--model_type chat \
--layers 12 \
--datasets all \
--device cuda:0
```